{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Simple Neural Network</h1><br><b>Our feed forward neural network is going to predict Xor operator. This code is very simple, just to ilustrate how neural networks work<b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # numpy because it rocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Activation function</h4> introduces non-linearity into our network. We use a sigmoid function $$f(x)=\\frac{1}{1+e^{-x}}$$<br>And along the line we will need its derivative $$f'(x)=x (1-x)$$ <br>This function is used to introduce non-linearity to the neural network. We can use other functions different than sigmoid, for example tanh.<br> We will see later how its being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(x,deriv=False):\n",
    "    if(deriv==False):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Inputs and Outputs</h4>\n",
    "Our data is just going to be binary vectors, and we are going to predict XOR function. We will feed 4 vectors in our Neural Network, and we want it to learn to predict the outputs.<br>\n",
    "So here we see our input, and our expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputarray = np.array([[0,0], \n",
    "                       [0,1],\n",
    "                       [1,1],\n",
    "                       [1,0]])\n",
    "\n",
    "outputarray = np.array([[0],\n",
    "                        [1],\n",
    "                        [0],\n",
    "                        [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Defining Neural Network</h4>\n",
    "Our Network is going to have 3 layers. Lots of examples show neural network ilustrating nodes and arrows. I think more helpful way to think about it is matricies. We have 3 matricies representing our layers, and 2 matricies representing our weights.\n",
    "<ul>\n",
    "  <li>Input layer - <b>layer1 - 4x2 Matrix </b></li>\n",
    "  <li>Hidden layer - <b>layer2  - 4x5 matrix</b></li>\n",
    "  <li>Output layer - <b>layer3  - 4x1 matrix</b></li>\n",
    "</ul>\n",
    "We also define weights between layers\n",
    "<ul>\n",
    "  <li>Weights between layer1 and layer2 - weights1 - <b>matrix 2x5</b></li>\n",
    "  <li>Weights between layer2 and layer3 - weights2 - <b>5x1</b></li>\n",
    "</ul>\n",
    "To save computational power, we dont feed the vectors in our neural network one by one, but simply feed all of them at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1) # seed is better for debugging, we generate same random numbers every time we run\n",
    "\n",
    "#defining weights and layers - all are matrixies (numpy arrays)\n",
    "weights1 = np.random.random((2,5)) # random for now\n",
    "weights2 = np.random.random((5,1)) \n",
    "layer1 = np.zeros((4,2)) \n",
    "layer2 = np.zeros((4,5))\n",
    "layer3 = np.zeros((4,1)) \n",
    "\n",
    "layer3_error = np.zeros((4,1)) # error term, saying how far our prediction is from a correct output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a Neural Net initialized and we can make a prediction ! (although a crappy one, because we have not trained our network yet).<br><b>What we do in the cell bellow this is called Forward propagation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75160406]\n",
      " [ 0.22170815]\n",
      " [-0.81373132]\n",
      " [ 0.20825635]]\n"
     ]
    }
   ],
   "source": [
    "layer1 = inputarray # we assign our input to layer1\n",
    "layer2 = np.dot(layer1,weights1) # We matrix multiply our first layer with the weights to get our second layer\n",
    "layer2 = activation(layer2) # But we also need to apply our activation function to our second layer\n",
    "# This activation function is simply applied to all the elements in the matrix\n",
    "\n",
    "layer3 = activation(np.dot(layer2,weights2)) # We do the same process to get our 3rd layer\n",
    "# now we have our crappy prediction in the layer3 variable\n",
    "layer3_error = outputarray - layer3\n",
    "print(layer3_error) # we can take a look how far are we from a correct result, this error rate is what we want to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction is useless, because the weights are random for now. We want to adjust the weights, so that our prediction is closer to our result. The weights our adjusted with gradient descent using back propagation. This process is repeated many times, in each step getting closer to our desired result.<br>\n",
    "<h4>Backpropagation</h4>\n",
    "So we have computed our error term (layer3_error) and we want to minimize it. For each element in our Output matrix (layer3) we compute the derivative of the sigmoid function (in that given point), multiply it by the error term to get layer3_delta, and finally updade the weights2 by the multiplication of layer3 delta and transpose of layer2 (as we can see in the code). Similar approach is used to update weights1 (and generaly other weights in NNs with more layers).\n",
    "<h4>Training</h4>\n",
    "We train the neural net by simply forward propagating, getting the error term, and then backpropagating with the error term and adjusting the weights. Here we use 50 000 steps, and write the error term every 5000 steps to see if its decreasing.<br>\n",
    "Tip : Run the cell multiple times to see how fast is the error rate decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.498824970753\n",
      "Error: 0.0240046602858\n",
      "Error: 0.0162288274875\n",
      "Error: 0.013027624303\n",
      "Error: 0.0111711562607\n",
      "Error: 0.00992267210558\n",
      "Error: 0.00900949548553\n",
      "Error: 0.00830426646812\n",
      "Error: 0.00773847060385\n",
      "Error: 0.0072715382651\n"
     ]
    }
   ],
   "source": [
    "for i in range(50000):\n",
    "    # this is just forward propagation that we already did\n",
    "    layer1=inputarray\n",
    "    layer2 = activation(np.dot(layer1,weights1))\n",
    "    layer3 = activation(np.dot(layer2,weights2))\n",
    "   \n",
    "    layer3_error = outputarray - layer3\n",
    "    # we get our error, like we did before\n",
    "   \n",
    "    # now we start backpropagation\n",
    "    layer3_delta = layer3_error*activation(layer3, deriv=True)\n",
    "\n",
    "    layer2_error = layer3_delta.dot(weights2.T)\n",
    "\n",
    "    layer2_delta = layer2_error * activation(layer2,deriv=True)\n",
    "\n",
    "    #update weights (no learning rate term)\n",
    "    weights2 += layer2.T.dot(layer3_delta)\n",
    "    weights1 += layer1.T.dot(layer2_delta)\n",
    "    \n",
    "    if(i % 5000) == 0:   # Print our error rate every 5000 steps \n",
    "        print(\"Error: \" + str(np.mean(np.abs(layer3_error))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our prediction is really close to our desired result. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00876447]\n",
      " [ 0.99350773]\n",
      " [ 0.0055496 ]\n",
      " [ 0.99329519]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(layer3) # prediction\n",
    "print(outputarray) # desired result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now want to predict output for a new vector, we can simply do it with forward propagation like this.\n",
    "We have used all the possible combinations of two binary values in our training data, so [1,0] is what the network was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99329526]\n"
     ]
    }
   ],
   "source": [
    "l1=np.array([1,0])\n",
    "l2 = activation(np.dot(l1,weights1))\n",
    "l3 = activation(np.dot(l2,weights2))\n",
    "print(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
